{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised WSD. Naïve Bayes Classification.\n",
    "*Author: Daniel Nova*\n",
    "\n",
    "## Background\n",
    "The Naïve-Bayes Classifier is a supervised machine learning model based on the Bayes Theorem with a strong independence assumption between the features (hence the \"naïve\" name). Based on the $n$ features of an observation $x$,  $\\vec{x}=(x_1,x_2,...,x_n)$, this classifier estimates to which class $c$ observation $x$ belongs to. We can compute the conditional probability of $x$ belonging to a class $c$ as:\n",
    "$$P(c|x)=\\frac{P(x|c)P(c)}{P(x)}$$\n",
    "\n",
    "* $P(x|c)$ being the **likelihood** of $x$ belonging to $c$.\n",
    "* $P(c)$ being the **prior** probability of class $c$.\n",
    "* $P(x)$ being the **evidence**.\n",
    "\n",
    "The goal of the classifier is to find the **best class**, maximum a-posteriori class (MAP),  given our observation.\n",
    "$$c_{MAP}=argmax_{c\\in C} P(c|x)$$\n",
    "\n",
    "Given that we use the Bayesian interpretation to compute this probability, and the denominator can be ignored because its value is the same for all possible $x$'s, our task becomes:\n",
    "$$c_{MAP}=argmax_{c\\in C} P(x_1,x_2,...,x_n|c)P(c)$$\n",
    "\n",
    "Based on the naïve feature independence assumption, the likelihood $P(x|c)$ can be formulated as:\n",
    "$$P(x|c) = \\prod_{i=1}^{n}P(x_i|c)$$\n",
    "With this new  interpretation, the  Naïve-Bayes Classifier can now be expressed as:\n",
    "$$c_{nb}=argmax_{c_j\\in C}  P(c_j) \\prod_{x \\in X} P(x|c)$$\n",
    "\n",
    "## Supervised Word Sense Disambiguation\n",
    "From a Machine Learning perspective WSD involves a **classification** task, we want to classify a word as belonging to a class that represents its sense. When we have a labeled corpus that contains the correct word senses, we can perform WSD using a supervised ML approach, to do it we extract useful features, train, and test a classifier.\n",
    "\n",
    "Based on the Naive Bayes formulation we did before, we interpret this classification problem as:\n",
    "$$\\hat{s}_{nb}=argmax_{s\\in S} P(s) \\prod_{i=1}^{n} P(x_i|s)$$\n",
    "Where:\n",
    "* $\\hat{s}$ is the estimated sense\n",
    "* $P(s_j)$ can be computed by counting the number of occurrences of $s_j$ divided by the count of the target word $w$ :\n",
    "\t$$P(s_j) = \\frac{count(s_j, w)}{count(w)}$$\n",
    "* $P(x_i|s)$ the likelihood of the set of features $x_i$ belonging to sense $s$. Which can be computed as: \n",
    "\t$$P(x_i|s) = \\frac{count(x_i|, s)}{count(s)} $$\n",
    "\n",
    "## Feature Selection\n",
    "As in any supervised machine learning task, it is crucial to select  useful features to predict word senses. For determining the sense we have to look at its context, in this case the surrounding text around a target word; from this context we can extract two kinds of features: *collocational* and *bag-of-words*.\n",
    "* *Collocational features* are taken from the words surrounding the target word, these are chosen based on a **window** to determine how many words to consider. To build the feature vector we can include the word, its root form, and its part of speech (POS). Using a window of two this vector takes the form:\n",
    "$$[w_{i-2}, POS_{i-2}, w_{i-1}, POS_{i-1}, w_{i+1}, POS_{i+1}, w_{i+2}, POS_{i+2}]$$\n",
    "\n",
    "e.g. of the surrounding words for text *guitar and bass player stand* for word *bass*\n",
    "``['guitar', 'NN', 'and', 'CC', 'player', 'NN', 'stand', 'VB']``\n",
    "\n",
    "* *Bag-of-words* we take into consideration the surrounding text for a given target word but do not care about their position, instead from a set context region we create an ordered word vector, excluding stopwords, and build the features. We want the counts of the words, to do it we use a **co-occurences** vector.\n",
    "\n",
    "We need to create a vector with the terms that co-occur the most for a given class in our corpus, which works as a *reference* vector. To build the features of an observation we use the most frequent words for that instance and build the vector based on the *reference* we created before, in other words if the most frequent terms in the instance occur in the *reference* vector we include this term in the co-occurrences vector.\n",
    "\n",
    "To give an example of a co-occurence vector, let's take a look at this example from Jurafsky and Martin's book:\n",
    "We have a 12 words vocuabluary of the most frequent words for **bass**:\n",
    "```\n",
    "[fishing, big, sound, player, fly, rod, pound, double, runs, playing, guitar, band] \n",
    "```\n",
    "So the co-occurence vector for a sentence *\"guitar and bass player stand\"* would be:\n",
    "$$[0,0,0,1,0,0,0,0,0,0,1,0]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking a look at the data\n",
    "Now we'll take a look at our Senseval corpus that we downloaded from the NLTK website. If you haven't downloaded it yet follow the instructions at <http://www.nltk.org/data.html>\n",
    "To begin let's import the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import senseval\n",
    "\n",
    "files_list = senseval.fileids()\n",
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus consists of 4 files, each containing senses for 4 different word tokens: **hard, interest, line**, and **serve**.\n",
    "Each instance consists of:\n",
    "* Target word in its root form. Marked with a ``<head>`` tag in the corpus file.\n",
    "* Senses: The ids of the senses of the target word. \n",
    "* Context. A sentence that contains the target word.\n",
    "* Position: Position of target word in the context\n",
    "* POS (part-of-speech) tags for each token.\n",
    "\n",
    "NLTK already has a corpus reader for Senseval data, you can learn more about NLTK's Corpus Readers at: <http://www.nltk.org/howto/corpus.html>. Let' check the first instance for *hard*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target word:  hard-a\n",
      "Position of the target word: 10\n",
      "Senses:  ('HARD1',)\n",
      "Context of the target word, includes POS tags: [('clever', 'NNP'), ('white', 'NNP'), ('house', 'NNP'), ('``', '``'), ('spin', 'VB'), ('doctors', 'NNS'), (\"''\", \"''\"), ('are', 'VBP'), ('having', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('helping', 'VBG'), ('president', 'NNP'), ('bush', 'NNP'), ('explain', 'VB'), ('away', 'RB'), ('the', 'DT'), ('economic', 'JJ'), ('bashing', 'NN'), ('that', 'IN'), ('low-and', 'JJ'), ('middle-income', 'JJ'), ('workers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('these', 'DT'), ('days', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "instance = senseval.instances('hard.pos')[1]\n",
    "\n",
    "# Let's check its fields\n",
    "print('Target word: ', instance.word)\n",
    "print('Position of the target word:', instance.position)\n",
    "print('Senses: ', instance.senses)\n",
    "print('Context of the target word, includes POS tags:', instance.context) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "instances = []\n",
    "labels_classes = []  # List of (word, sense) tuples\n",
    "\n",
    "\n",
    "# LOAD DATA AND SET OF LABELS (CLASSES)\n",
    "for file in files_list:\n",
    "    for i in senseval.instances(file):\n",
    "        senses = i.senses\n",
    "\n",
    "        # I had issues with the 'lazy' data loading approach, so I just copied the data directly into memory\n",
    "        temp_instance = copy.deepcopy(i)  \n",
    "        \n",
    "        for sense in senses:  # Note that an instance may have more than one sense\n",
    "            temp_instance.senses = sense\n",
    "            instances.append(temp_instance)  # We create a separate entry for each sense\n",
    "            labels_classes.append((i.word, sense))\n",
    "label_sets = set(labels_classes)  # create our set of possible classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ``label_sets`` as a set of the word senses in our data. We see that **hard** has 3 senses, **interest** has 6, **line** also has 6, and **serve** has 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('hard-a', 'HARD1'),\n",
       " ('hard-a', 'HARD2'),\n",
       " ('hard-a', 'HARD3'),\n",
       " ('interest-n', 'interest_1'),\n",
       " ('interest-n', 'interest_2'),\n",
       " ('interest-n', 'interest_3'),\n",
       " ('interest-n', 'interest_4'),\n",
       " ('interest-n', 'interest_5'),\n",
       " ('interest-n', 'interest_6'),\n",
       " ('line-n', 'cord'),\n",
       " ('line-n', 'division'),\n",
       " ('line-n', 'formation'),\n",
       " ('line-n', 'phone'),\n",
       " ('line-n', 'product'),\n",
       " ('line-n', 'text'),\n",
       " ('serve-v', 'SERVE10'),\n",
       " ('serve-v', 'SERVE12'),\n",
       " ('serve-v', 'SERVE2'),\n",
       " ('serve-v', 'SERVE6')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "We define two important variables, ``context``: the amount of words that are taken for each instance to analyze frequency (set to 2), and ``num_co_ocurrences``: the number of lemmas that co-occur with the target word, which is set to 12. Play around with these values to see how the results change.\n",
    "\n",
    "We need to define a couple of functions.\n",
    "``tokenize(text)`` function just tokenizes a text using the WordPunctTokenizer. We use it to get rid of punctuation and apostrophes. It may seem unnecessary, but we will re-use this function later when we'll use non-POS tagged data ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = 2\n",
    "num_co_ocurrences = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "filters = [\"``\", \"''\"]  # we want to filter these from our data\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "translator = str.maketrans('', '', string.punctuation)  # used to remove punctuation\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenized_text = []\n",
    "    for w in text:\n",
    "        word = tokenizer.tokenize(w[0])[0].lower() # remove punctuation present in the text\n",
    "        norm_word = word.translate(translator)                \n",
    "        if len(norm_word) >= 1: \n",
    "            tokenized_text.append((norm_word, w[1]))  # returns tuple (word, POS)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define ``extract_context``. This function takes the surrounding tokens for a given position. So for a text *\"I don't really like red apples\"* for position 4, with a ``context=2`` the surrounding words are ``['really','like','apples']``.\n",
    "\n",
    "The function returns the context to the left and right of the target word as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_context(target_position, text, context):\n",
    "    # extract target word\n",
    "    text_wo_target_left = [t for t in text[:target_position]]\n",
    "    text_wo_target_right = [t for t in text[target_position+1:]]\n",
    "    \n",
    "    # tokenize\n",
    "    text_wo_target_left = tokenize(text_wo_target_left)\n",
    "    text_wo_target_right = tokenize(text_wo_target_right)\n",
    "    \n",
    "    # return extracted context\n",
    "    return text_wo_target_left[-context:], text_wo_target_right[:context]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with our data, we want to get the context of *'house'* with a window of 2:\n",
    "Note that we removed punctutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('clever', 'NNP'), ('white', 'NNP')], [('spin', 'VB'), ('doctors', 'NNS')])\n"
     ]
    }
   ],
   "source": [
    "print(extract_context(2, instance.context, context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to create the features vector, ``extract_features``.\n",
    "\n",
    "We create the vector using the target word, its context (surrounding words), the POS tags of the context, and the co-occurence vector (that we will show next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(targetword, left_ctxt, right_ctxt, co_occ_vector):\n",
    "    context_text = ([i[0] for i in left_ctxt] + [i[0] for i in right_ctxt])\n",
    "    context_pos = ([i[1] for i in left_ctxt] + [i[1] for i in right_ctxt])\n",
    "    co_occurence_features = [1 if i in context_text else 0 for i in co_occ_vector]\n",
    "    \n",
    "    return {'word': targetword.split('.')[0],\n",
    "            'co-occ': tuple(co_occurence_features),\n",
    "            'context': tuple(context_text), \n",
    "            'pos_context': tuple(context_pos)\n",
    "            }\n",
    "\n",
    "    #  e.g. of a feature vector  \n",
    "    # {'co-occ': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "    # 'context': ('toll', 'telephone', 'and', 'conduct')},\n",
    "    # 'pos_context': ('JJ', 'NN', 'CC', 'NN')\n",
    "    # 'word': 'lines'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Co-occurence vector\n",
    "With these defined functions we now can compute a vector of the most frequent content words in the collection for each class, we use ``context`` to determine how many words from the left and right of the target word to take, and ``num_co_occurrences`` to determine the number of most frequent words to consider. To store this information we use a dictionary variable named ``co_occurences_per_classword`` using the lemma of the target-word as key and the vector as value. \n",
    "\n",
    "The co-occurrence vector is built using ``co_occurences_per_classword`` as reference, adding 1 if each term in the surrounding context is found on the reference, 0 otherwise. \n",
    "\n",
    "For example, the most common co-occurences for **bitter** are:\n",
    "```\n",
    "'hard-a': ['time','find','work', 'believe', 'get', 'make', 'imagine', 'look', 'say', 'way', 'would', 'take']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "num_co_ocurrences = 12\n",
    "co_occurrences_per_classword = defaultdict(dict)\n",
    "\n",
    "#  Train and Test set.\n",
    "#  For testing classification on single classes we use separate sets for each class\n",
    "test_set_per_class = defaultdict(dict)\n",
    "train_set_per_class = defaultdict(dict)\n",
    "\n",
    "# Get the most co-occurrences of lemmas to every class (excluding sense)\n",
    "for classword in set([l[0] for l in label_sets]):\n",
    "    co_occurrences_per_classword[classword] = []  # create entry in the dictionary\n",
    "    lemmas_for_class = []  #  All possible lemmas for the class\n",
    "    for instance in instances:\n",
    "        #  get all text for given label\n",
    "        if classword == instance.word:\n",
    "            \n",
    "            # get context on the left and right\n",
    "            left_context, right_context = extract_context(instance.position, \n",
    "                                                          instance.context, context)\n",
    "            \n",
    "            # i[0] indicates we only take the first element of the tuple (word, pos)\n",
    "            # We don't need the POS tags to build the co-occurrences vector.\n",
    "            # We also filter stopwords.\n",
    "            lemmas_for_class += [i[0] for i in left_context\n",
    "                                 if i[0] not in stopwords.words('english')\n",
    "                                 and len(i[0]) > 1]\n",
    "            lemmas_for_class += [i[0] for i in right_context \n",
    "                                 if i[0] not in stopwords.words('english')\n",
    "                                 and len(i[0]) > 1]\n",
    "\n",
    "    # Compute most frequent occurrences\n",
    "    dist = FreqDist(lemmas_for_class)  # Build the frequency disribution\n",
    "\n",
    "    # Only take into account the most common occurrences (remember we initially set num_co_ocurrences = 12)\n",
    "    cooccurrences = [l[0] for l in dist.most_common(num_co_ocurrences)]\n",
    "    \n",
    "    # Save this vector for this class\n",
    "    co_occurrences_per_classword[classword] = cooccurrences[:num_co_ocurrences]\n",
    "\n",
    "    train_set_per_class[classword] = []  # initialize\n",
    "    test_set_per_class[classword] = []  # initialize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our results for word class **hard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'find',\n",
       " 'work',\n",
       " 'believe',\n",
       " 'get',\n",
       " 'make',\n",
       " 'imagine',\n",
       " 'look',\n",
       " 'say',\n",
       " 'way',\n",
       " 'would',\n",
       " 'take']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrences_per_classword['hard-a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our feature set (finally)\n",
    "Now we can finally build our set of features. For each word class and its meanings (named here using the `label` variable) we extract the features for each instance and store them in the `feature_sets` list.\n",
    "Once our feature set is completed, we store it into the set we are going to use to train or Naïve Bayes classifier. We'll use 80% of the data for training the classifier, and the remaining 20% to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for label in label_sets:\n",
    "    feature_sets = [] \n",
    "    label_word = label[0]  # e.g. line-n\n",
    "    label_sense = label[1]  # e.g. cord, division, formation, ...\n",
    "    for instance in instances:\n",
    "        if label_word == instance.word and label_sense in instance.senses:\n",
    "            word = instance.context[instance.position][0]  # take only the word, not the POS\n",
    "            left, right = (extract_context(instance.position, instance.context, context))\n",
    "            feature_sets += [(extract_features(\n",
    "                    word, left, right, \n",
    "                    co_occurrences_per_classword[label_word]), label)]\n",
    "            #  e.g. of a feature:  \n",
    "            # ({'co-occ': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "            # 'context': ('toll', 'telephone', 'and', 'conduct')},\n",
    "            # 'pos_context': ('JJ', 'NN', 'CC', 'NN')}),\n",
    "            # ('line.n': 'phone'))  <---- this is the label (class) with the meaning\n",
    "    \n",
    "    # Shuffle the instances to distribute them on the train and test sets\n",
    "    random.shuffle(feature_sets)\n",
    "    \n",
    "    #  We use 80% of the data for training, and 20% for testing\n",
    "    sample = int(len(feature_sets)*0.2)\n",
    "    \n",
    "    #  For the purpose of testing classification on single classes, we use separate sets for each class\n",
    "    #  For general classification, merge train_set_per_class and test_set_per_class\n",
    "    \n",
    "    training_set = feature_sets[sample+1:]\n",
    "    train_set_per_class[label_word] += training_set\n",
    "    test_set = feature_sets[:sample] \n",
    "    test_set_per_class[label_word] += test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of features that we have distributed into `train_set_per_class` and `test_set_per_class`, let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'co-occ': (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       "  'context': ('a', 'telephone', 'open', 'to'),\n",
       "  'pos_context': ('DT', 'NN', 'JJ', 'TO'),\n",
       "  'word': 'line'},\n",
       " ('line-n', 'phone'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_per_class['line-n'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an instance of **line**, where the word **telephone** also appears in the most common words for that class:\n",
    "\n",
    "(Remember that the data is shuffled, when you run it again it's going to be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'telephone',\n",
       " 'personal',\n",
       " 'long',\n",
       " 'computer',\n",
       " 'car',\n",
       " 'ibm',\n",
       " 'fine',\n",
       " 'draw',\n",
       " 'computers',\n",
       " 'one',\n",
       " 'company']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrences_per_classword['line-n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing our classifier\n",
    "\n",
    "Now that we have created our set of features and distributed them into training and test sets we can train our classifier using NLTK's `NaiveBayesClassifier`.\n",
    "\n",
    "First let's train a classifier to disambiguate **hard** and see how accurate it is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.792147806004619\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set_per_class['hard-a'])  #  Train for 'hard'\n",
    "\n",
    "#  Now we can check the classifier accuracy using our remaining test data\n",
    "print(\"Accuracy is: %s\" % nltk.classify.accuracy(classifier, test_set_per_class['hard-a']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 79%.\n",
    "\n",
    "Now let's create a classifier using all the data of the corpus, that is using **hard, line, interest, serve**. To do it we merge the `train_set_per_class` and `test_set_per_class` that we built, so we can use all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier including all words is: 0.7142387372574811\n"
     ]
    }
   ],
   "source": [
    "#  Merge train_set_per_class and test_set_per_class for classification on all classes.\n",
    "#  We can use train_set_per_class[key] to test on a specific class (specific word/senses)\n",
    "sub_train_set = []\n",
    "for key in train_set_per_class.keys():\n",
    "    sub_train_set += [i for i in train_set_per_class[key]]\n",
    "sub_test_set = []\n",
    "for key in test_set_per_class.keys():\n",
    "    sub_test_set += [i for i in test_set_per_class[key]]\n",
    "total = sub_train_set + sub_test_set\n",
    "\n",
    "random.shuffle(total)\n",
    "divide = int(len(total)*0.2)\n",
    "\n",
    "train_set = total[divide:]\n",
    "test_set = total[:divide]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of the classifier including all words is: %s\" % nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## To wrap up\n",
    "We were able to build a classifier that got us an accuracy of 71%.\n",
    "In the next section we sill further evaluate the performance of the classifier, we'll perform **cross-validation** and also use a confusion matrix to compute Precision and Recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
